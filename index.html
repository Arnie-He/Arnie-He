<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arnie He</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="navbar">
        <h3>Naicheng He(Arnie)</h3>
        <div>
            <a href="#Research">Research</a>
            <a href="#teaching">TA</a>
            <a href="projects.html">Projects</a>
        </div>
    </div>
    <div class="container">
        <div class="content">
            <div class="intro-section" id="about">
                <div class="intro-content">
                    <p>
                        I'm a 4th-year undergraduate student pursuing degrees in Computer Science and Mathematics at Brown University, 
                        where I do research advised by <a href="https://scholar.google.com/citations?user=aQCRuYQAAAAJ&hl=en" target="_blank">Professor Amy Greenwald</a> and 
                        <a href="https://cs.brown.edu/people/gdk/" target="_blank">Professor George Konidaris</a>.
                        
                        My main research interest lies in automated decision making systems with a focus on reinforcement learning and multi-agent interactions.
                        
                        <br>
                        <br>
                        Outside of research, I enjoy any competitive sports like tennis and soccer. I also enjoy strategy games like go and the civilization series.
                        <br>
                    </p>
                    <div class="social-links">
                        <p>
                            <a href="https://github.com/Arnie-He" target="_blank">
                                <i class="fab fa-github"></i> GitHub
                            </a>
                        </p>
                        <p>
                            <a href="https://www.linkedin.com/in/arniehe/" target="_blank">
                                <i class="fab fa-linkedin"></i> LinkedIn
                            </a>
                        </p>
                        <p id="contact">email: arnie_he@brown.edu</p>
                    </div>
                </div>
                <div class="profile-photo">
                    <img src="Profile_2.jpg" alt="Profile">
                </div>
            </div>

            <!-- <div class="News" id="News">
                <h2>News</h2>
                <p></p>
            </div> -->
            <div class="Selected Research" id="Research">
                <h2>Research</h2>
                <div class="research-item research-with-media">
                    <div class="research-media">
                        <video autoplay muted loop playsinline>
                            <source src="irl.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <div class="research-content">
                        <h3>Inverse Reinforcement Learning on GPUDrive</h3>
                        <p><em>In progress</em> - NYRL workshop 2025</p>
                        <p class="author"><strong>Naicheng He</strong>, Arjun Prakash, Gokul Swamy, Amy Greenwald, Eugene Vinitsky</p>
                        <p>We explore the use of inverse reinforcement learning (IRL) to develop robust driving policies in GPUDrive. 
                            Using demonstrations from either human experts or PPO-trained agents, 
                            we investigate GAIL-style approaches with PPO as inner-loop optimizers, 
                            and discriminators trained on egocentric observations or observation-action pairs. 
                            Our experiments span 75 worlds with varying numbers of controlled agents and we investigated the difficulty of scaling IRL from single-agent to
                             multi-agent environments. We evaluate policies using task-relevant metrics such as off-road counts, 
                             collisions, goal-reaching rates, and hand-crafted episodic returns. 
                             These early results raise intriguing questions about reward generalization, scalability, 
                             and the design of efficient algorithms for multi-agent autonomy.</p>
                    </div>
                </div>
                <div class="research-item">
                    <h3>Mitigating Loss of Plasticity by Preventing Hessian Spectral Collapse</h3>
                    <p><em>Under review</em> - Neurips ARLET workshop 2025</p>
                    <p class="author"><strong>Naicheng He*</strong>, Kaicheng Guo*, Arjun Prakash*, Saket Tiwari, Tyrone Serapio, Ruo Yu Tao, Amy Greenwald, George Konidaris</p>
                    <p>Deep neural networks suffer from the loss of plasticity, gradually losing their ability to adapt to new tasks. We identify the root cause of this phenomenon as the Hessian spectral collapse, 
                        where meaningful curvature directions vanish and optimization becomes ineffective. 
                        We conduct extensive experiments to find that spectral collapse perfectly aligns with the onset of plasticity loss. 
                        To explain this, we introduce the notion of τ-trainability, which unifies prior indicators and provides a principled characterization of plasticity. 
                        Building on this framework, we discuss the Kroner factored approximation which motivates two regularization choices: preserving the effective feature rank and applying 
                        L2 penalties, both of which directly counteract spectral collapse. 
                        Experiments on continual supervised and reinforcement learning tasks confirm that this theoretically motivated strategy effectively preserves plasticity for deep neural networks.</p>
                </div>
                <div class="research-item">
                    <h3>Bi-Level Policy Optimization with Nyström Hypergradients</h3>
                    <p><em>Under review</em> - <a href="https://arxiv.org/abs/2505.11714" target="_blank">arXiv</a></p>
                    <p class="author">Arjun Prakash*, <strong>Naicheng He*</strong>, Denizalp Goktas, Amy Greenwald</p>
                    <p>The dependency of the actor on the critic in actor-critic (AC) reinforcement learning means that AC can be characterized as a bilevel optimization (BLO) problem, also called a Stackelberg game. This characterization motivates two modifications to vanilla AC algorithms. First, the critic's update should be nested to learn a best response to the actor's policy. Second, the actor should update according to a hypergradient that takes changes in the critic's behavior into account. Computing this hypergradient involves finding an inverse Hessian vector product, a process that can be numerically unstable. We thus propose a new algorithm, Bilevel Policy Optimization with Nyström Hypergradients (BLPO), which uses nesting to account for the nested structure of BLO, and leverages the Nyström method to compute the hypergradient. Theoretically, we prove BLPO converges to (a point that satisfies the necessary conditions for) a local strong Stackelberg equilibrium in polynomial time with high probability, assuming a linear parametrization of the critic's objective. Empirically, we demonstrate that BLPO performs on par with or better than PPO on a variety of discrete and continuous control tasks.

                    </p>
                </div>
                <p style="text-align: right; margin-top: 15px;">
                    <a href="projects.html" style="color: #0077b5; text-decoration: none; font-size: 13px;">→ View More Projects I've Worked On</a>
                </p>
            </div>

            <div class="teaching" id="teaching">
                <h2>TA Experience</h2>
                <ul class="ta-list">
                    <li>CSCI 1470: Deep Learning (Fall 2023)</li>
                    <li>CSCI 1440: Algorithmic Game Theory (Spring 2024)</li>
                </ul>
            </div>
        </div>
    </div>
</body>
</html>
