<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arnie He</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="navbar">
        <div class="navbar-content">
            <h3><a href="index.html">Naicheng He(Arnie)</a></h3>
            <div>
                <a href="#Research">Research</a>
                <a href="#TA">TA</a>
                <a href="projects.html">Projects</a>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="content">
            <div class="intro-section" id="about">
                <div class="intro-content">
                    <p>
                        I'm a 4th-year undergraduate student pursuing degrees in Computer Science and Mathematics at Brown University, 
                        where I do research advised by <a href="https://scholar.google.com/citations?user=aQCRuYQAAAAJ&hl=en" target="_blank">Professor Amy Greenwald</a> and 
                        <a href="https://cs.brown.edu/people/gdk/" target="_blank">Professor George Konidaris</a>.
                        
                        My main research interest lies in automated decision making systems with a focus on reinforcement learning and multi-agent interactions.
                        
                        <br>
                        <br>
                        Outside of research, I enjoy any competitive sports, i.e. tennis and soccer. I also enjoy strategic games including go and the civilization series.
                        <br>
                    </p>
                    <div class="social-links">
                        <p>
                            <a href="https://github.com/Arnie-He" target="_blank">
                                <i class="fab fa-github"></i> GitHub
                            </a>
                        </p>
                        <p>
                            <a href="https://www.linkedin.com/in/arniehe/" target="_blank">
                                <i class="fab fa-linkedin"></i> LinkedIn
                            </a>
                        </p>
                        <p>
                            <a href="Naicheng-He(Arnie).pdf" target="_blank">
                                <i class="fas fa-file-pdf"></i> CV
                            </a>
                        </p>
                        <p id="contact">email: arnie_he@brown.edu</p>
                    </div>
                </div>
                <div class="profile-photo">
                    <img src="Profile_2.jpg" alt="Profile">
                </div>
            </div>

            <!-- <div class="News" id="News">
                <h2>News</h2>
                <p></p>
            </div> -->
            <div class="Selected Research" id="Research">
                <h2>Research</h2>

                <div class="research-item">
                    <h3>Mitigating Loss of Plasticity by Preventing Hessian Spectral Collapse</h3>
                    <p style="font-size: 14px; color: #666; margin: 5px 0;">To present at Neurips ARLET workshop 2025</p>
                    <p><em>Under review</em> - <a href="https://arxiv.org/abs/2509.22335" target="_blank">[paper]</a> <a href="https://github.com/KevinGuo27/lop-jax" target="_blank">[code]</a> </p> 
                    <p class="author"><strong>Naicheng He*</strong>, Kaicheng Guo*, Arjun Prakash*, Saket Tiwari, Tyrone Serapio, Ruo Yu Tao, Amy Greenwald, George Konidaris</p>
                    <p>We investigate why deep neural networks suffer from loss of plasticity in deep continual learning, failing to learn new tasks without reinitializing parameters.
                    We show that this failure is preceded by Hessian spectral collapse at new-task initialization, where meaningful curvature directions vanish and gradient descent becomes ineffective. 
                    To characterize the necessary condition for successful training, we introduce the notion of τ-trainability and show that current plasticity preserving algorithms can be unified under this framework. 
                    Targeting spectral collapse directly, we then discuss the Kronecker factored approximation of the Hessian, which motivates two regularization enhancements: maintaining high effective feature rank and applying L2 penalties. 
                    Experiments on continual supervised and reinforcement learning tasks confirm that combining these two regularizers effectively preserves plasticity.</p>
                </div>

                <div class="research-item research-with-media">
                    <div class="research-media">
                        <video autoplay muted loop playsinline>
                            <source src="irl.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <div class="research-content">
                        <h3>Inverse Reinforcement Learning on GPUDrive</h3>
                        <p style="font-size: 14px; color: #666; margin: 5px 0;">Presented at NYRL workshop 2025</p>
                        <p><em>In progress</em> - <a href="nyrl_submission_gpudrive (5).pdf" target="_blank">[write-up]</a> <a href="https://github.com/Arnie-He/gpudrive/tree/base_gail" target="_blank">[code]</a> <a href="InverseRL_on_gpudrive (1).pdf" target="_blank">[poster]</a></p>
                        <p class="author"><strong>Naicheng He</strong>, Arjun Prakash, Gokul Swamy, Amy Greenwald, Eugene Vinitsky</p>
                        <p>We explore the use of inverse reinforcement learning (IRL) to develop robust driving policies in GPUDrive. 
                            Using demonstrations from either human experts or PPO-trained agents, 
                            we investigate GAIL-style approaches with PPO as inner-loop optimizers, 
                            and discriminators trained on egocentric observations or observation-action pairs. 
                            Our experiments span 75 worlds with varying numbers of controlled agents and we investigated the difficulty of scaling IRL from single-agent to
                            multi-agent environments. We evaluate policies using task-relevant metrics such as off-road counts, 
                            collisions, goal-reaching rates, and hand-crafted episodic returns. 
                            These early results raise intriguing questions about reward generalization, scalability, 
                            and the design of efficient algorithms for multi-agent autonomy.</p>
                    </div>
                </div>
                
                <div class="research-item">
                    <h3>Bi-Level Policy Optimization with Nyström Hypergradients</h3>
                    <!-- <p style="font-size: 14px; color: #666; margin: 5px 0;">arXiv</p> -->
                    <p><em>Under review</em> - <a href="https://arxiv.org/abs/2505.11714" target="_blank">[paper]</a> <a href="https://github.com/Arnie-He/BLPO" target="_blank">[code]</a> <a href="BLPO_Poster.pdf" target="_blank">[poster]</a></p>
                    <p class="author">Arjun Prakash*, <strong>Naicheng He*</strong>, Denizalp Goktas, Amy Greenwald</p>
                    <p>The dependency of the actor on the critic in actor-critic (AC) reinforcement learning means that AC can be characterized as a bilevel optimization (BLO) problem, also called a Stackelberg game. This characterization motivates two modifications to vanilla AC algorithms. First, the critic's update should be nested to learn a best response to the actor's policy. Second, the actor should update according to a hypergradient that takes changes in the critic's behavior into account. Computing this hypergradient involves finding an inverse Hessian vector product, a process that can be numerically unstable. We thus propose a new algorithm, Bilevel Policy Optimization with Nyström Hypergradients (BLPO), which uses nesting to account for the nested structure of BLO, and leverages the Nyström method to compute the hypergradient. Theoretically, we prove BLPO converges to (a point that satisfies the necessary conditions for) a local strong Stackelberg equilibrium in polynomial time with high probability, assuming a linear parametrization of the critic's objective. Empirically, we demonstrate that BLPO performs on par with or better than PPO on a variety of discrete and continuous control tasks.

                    </p>
                </div>
                <p style="text-align: right; margin-top: 15px;">
                    <a href="projects.html" style="color: #0077b5; text-decoration: none; font-size: 13px;">→ View More Projects I've Worked On</a>
                </p>
            </div>

            <div class="teaching" id="TA">
                <h2>TA Experience</h2>
                <ul class="ta-list">
                    <li>CSCI 1470: Deep Learning (Fall 2023)</li>
                    <li>CSCI 1440: Algorithmic Game Theory (Spring 2024)</li>
                </ul>
            </div>
        </div>
    </div>
</body>
</html>
